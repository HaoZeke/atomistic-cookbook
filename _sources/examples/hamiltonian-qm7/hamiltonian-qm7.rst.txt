
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/hamiltonian-qm7/hamiltonian-qm7.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_examples_hamiltonian-qm7_hamiltonian-qm7.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_hamiltonian-qm7_hamiltonian-qm7.py:


Hamiltonian Learning for Molecules with Indirect Targets
========================================================

:Authors: Divya Suman `@DivyaSuman14 <https://github.com/DivyaSuman14>`__,
          Hanna Tuerk `@HannaTuerk <https://github.com/HannaTuerk>`__

This tutorial introduces a machine learning (ML) framework
that predicts Hamiltonians for molecular systems. Another
one of our `cookbook examples
<https://atomistic-cookbook.org/examples/periodic-hamiltonian/periodic-hamiltonian.html>`__
demonstrates an ML model that predicts real-space Hamiltonians
for periodic systems. While we use the same model here to predict
a molecular Hamiltonians, we further finetune these models to
optimise predictions of different quantum mechanical (QM)
properties of interest, thereby treating the Hamiltonian
predictions as an intermediate component of the ML
framework. More details on this hybrid or indirect learning
framework can be found in `ACS Cent. Sci. 2024, 10, 637−648.
<https://pubs.acs.org/doi/full/10.1021/acscentsci.3c01480>`_
and our preprint `arXiv:2504.01187 <https://doi.org/10.48550/arXiv.2504.01187>`_.

.. GENERATED FROM PYTHON SOURCE LINES 26-35

Within a Hamiltonian learning framework, one could chose to learn
a target that corresponds to the matrix representation for an
existing electronic structure method, but in a finite AO basis,
such a representation will lead to a finite basis set error.
The parity plot below illustrates this error by showing the
discrepancy between molecular orbital (MO) energies of an ethane
molecule obtained from a self-consistent calculation on a minimal
STO-3G basis and the larger def2-TZVP basis, especially for the
high energy, unoccupied MOs.

.. GENERATED FROM PYTHON SOURCE LINES 37-41

.. figure:: minimal_vs_lb.png
   :alt: Parity plot comparing the MO energies of ethane from a
          DFT calculation with the STO-3G and the def2-TZVP basis.
   :width: 600px

.. GENERATED FROM PYTHON SOURCE LINES 43-54

The choice of basis set plays a crucial role in determining
the accuracy of the observables derived from the predicted
Hamiltonian. Although the larger basis sets generally provide
more reliable results, the computational cost to compute the electronic
structure in a larger basis or to train such a
model is notably higher compared to a smaller basis.
Using the indirect learning framework, one
could instead learn a reduced effective Hamiltonian that reproduces
calculations from a much larger basis while using a significantly
simpler and smaller model consistent with a smaller basis.


.. GENERATED FROM PYTHON SOURCE LINES 56-62

We first show an example where we predict the reduced effective
Hamiltonians for a homogenous dataset of ethane molecule while
targeting the MO energies of the def2-TZVP basis. In a second example
we will then target multiple properties for a organic molecule dataset,
similar to our results described in our preprint
`arXiv:2504.01187 <https://doi.org/10.48550/arXiv.2504.01187>`_.

.. GENERATED FROM PYTHON SOURCE LINES 64-66

1. Example of Learning MO Energies for Ethane
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 68-70

Python Environment and Used Packages
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. GENERATED FROM PYTHON SOURCE LINES 72-77

We start by creating a virtual environment and installing
all necessary packages. The required packages are provided
in the environment.yml file that can be dowloaded at the end.
We can then import the necessary packages.


.. GENERATED FROM PYTHON SOURCE LINES 77-105

.. code-block:: Python


    import os

    import matplotlib.pyplot as plt
    import numpy as np
    import torch
    from ase.units import Hartree
    from IPython.utils import io
    from mlelec.features.acdc import compute_features_for_target
    from mlelec.targets import drop_zero_blocks  # noqa: F401
    from mlelec.utils.plot_utils import plot_losses


    os.environ["PYSCFAD_BACKEND"] = "torch"
    import mlelec.metrics as mlmetrics  # noqa: E402
    from mlelec.data.dataset import MLDataset, MoleculeDataset, get_dataloader  # noqa: E402
    from mlelec.models.linear import LinearTargetModel  # noqa: E402
    from mlelec.train import Trainer  # noqa: E402
    from mlelec.utils.property_utils import (  # noqa: E402
        compute_dipole_moment,
        compute_eigvals,
        compute_polarisability,
        instantiate_mf,
    )


    torch.set_default_dtype(torch.float64)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/atomistic-cookbook/atomistic-cookbook/.nox/hamiltonian-qm7/lib/python3.11/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
      warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
    Using PyTorch backend.




.. GENERATED FROM PYTHON SOURCE LINES 106-113

Set Parameters for Training
^^^^^^^^^^^^^^^^^^^^^^^^^^^
Before we begin our training we can decide on a set the parameters,
including the dataset set size, splitting fractions, the batch size,
learning rate, number of epochs, and the early stop criterion in case of
early stopping.


.. GENERATED FROM PYTHON SOURCE LINES 113-132

.. code-block:: Python


    NUM_FRAMES = 100
    BATCH_SIZE = 4
    NUM_EPOCHS = 100  # 100
    SHUFFLE_SEED = 42
    TRAIN_FRAC = 0.7
    TEST_FRAC = 0.1
    VALIDATION_FRAC = 0.2
    EARLY_STOP_CRITERION = 20
    VERBOSE = 10
    DUMP_HIST = 50
    LR = 1e-1  # 5e-4
    VAL_INTERVAL = 1
    DEVICE = "cpu"

    ORTHOGONAL = True  # set to 'FALSE' if working in the non-orthogonal basis
    FOLDER_NAME = "output/ethane_eva"
    NOISE = False








.. GENERATED FROM PYTHON SOURCE LINES 133-139

Create Folders and Save Parameters
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We can save the parameters we just defined for our reference later.
For this, we create a folder (defined as FOLDER_NAME above)
in which all parameters
and the generated data for this example are stored.

.. GENERATED FROM PYTHON SOURCE LINES 139-167

.. code-block:: Python


    os.makedirs(FOLDER_NAME, exist_ok=True)
    os.makedirs(f"{FOLDER_NAME}/model_output", exist_ok=True)


    def save_parameters(file_path, **params):
        with open(file_path, "w") as file:
            for key, value in params.items():
                file.write(f"{key}: {value}\n")


    # Call the function with your parameters
    save_parameters(
        f"{FOLDER_NAME}/parameters.txt",
        NUM_FRAMES=NUM_FRAMES,
        BATCH_SIZE=BATCH_SIZE,
        NUM_EPOCHS=NUM_EPOCHS,
        SHUFFLE_SEED=SHUFFLE_SEED,
        TRAIN_FRAC=TRAIN_FRAC,
        TEST_FRAC=TEST_FRAC,
        VALIDATION_FRAC=VALIDATION_FRAC,
        LR=LR,
        VAL_INTERVAL=VAL_INTERVAL,
        DEVICE=DEVICE,
        ORTHOGONAL=ORTHOGONAL,
        FOLDER_NAME=FOLDER_NAME,
    )








.. GENERATED FROM PYTHON SOURCE LINES 168-182

Generate Reference Data
^^^^^^^^^^^^^^^^^^^^^^^

In principle one can generate the training data of reference
Hamiltonians from a given set of structures, using any
electronic structure code. Here we provide a pre-computed,
homogenous dataset that contains 100 different
configurations of ethane molecule. For all structures, we
performed Kohn-Sham density functional theory (DFT)
calculations with `PySCF <https://github.com/pyscf/pyscf>`_,
using the B3LYP functional. For each molecular geometry, we
computed the Fock and overlap matrices along with other
molecular properties of interest, using both STO-3G and def2-TZVP
basis sets.

.. GENERATED FROM PYTHON SOURCE LINES 184-214

Prepare the Dataset for ML Training
"""""""""""""""""""""""""""""""""""

In this section, we will prepare the dataset required to
train our machine learning model using the ``MoleculeDataset``
and ``MLDataset`` classes. These classes help format and store
the DFT data in a way compatible with our ML package,
`mlelec  <https://github.com/curiosity54/mlelec/tree/qm7>`_.
In this section we initialise the ``MoleculeDataset`` where
we specify the molecule name, file paths and the desired targets
and auxillary data to be used for training for the minimal
(STO-3G), as well as a larger basis (lb, def2-TZVP).
Once the molecular data is prepared, we wrap it into an
``MLDataset`` instance. This class structures the dataset
into a format that is optimal for ML the Hamiltonians. The
Hamiltonian matrix elements depend on specific pairs of
orbitals involved in the interaction. When these orbitals
are centered on atoms, as is the case for localized AO
bases, the Hamiltonian matrix elements can be viewed
as objects labeled by pairs of atoms, as well as multiple
quantum numbers, namely the radial (`n`) and the angular
(`l`, `m`) quantum numbers characterizing each AO. These
angular functions are typically chosen to be real spherical
harmonics, and determine the equivariant behavior of the
matrix elements under rotations and inversions. ``MLDataset``
leverages this equivariant structure of the Hamiltonians,
which is discussed in further detail in the `Periodic Hamiltonian Model Example
<https://atomistic-cookbook.org/examples/periodic-hamiltonian/periodic-hamiltonian.html>`__
. Finally, we split the loaded dataset into training,
validation and test datasets using ``_split_indices``.

.. GENERATED FROM PYTHON SOURCE LINES 214-241

.. code-block:: Python


    molecule_data = MoleculeDataset(
        mol_name="ethane",
        use_precomputed=False,
        path="data/ethane/",
        aux_path="data/ethane/sto-3g",
        frame_slice=slice(0, NUM_FRAMES),
        device=DEVICE,
        aux=["overlap", "orbitals"],
        lb_aux=["overlap", "orbitals"],
        target=["fock", "eva"],
        lb_target=["fock", "eva"],
    )

    ml_data = MLDataset(
        molecule_data=molecule_data,
        device=DEVICE,
        model_strategy="coupled",
        shuffle=True,
        shuffle_seed=SHUFFLE_SEED,
        orthogonal=ORTHOGONAL,
    )

    ml_data._split_indices(
        train_frac=TRAIN_FRAC, val_frac=VALIDATION_FRAC, test_frac=TEST_FRAC
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading structures
    data/ethane/sto-3g/fock.hickle
    data/ethane/sto-3g/eva.hickle
    data/ethane/def2-tzvp/fock.hickle
    data/ethane/def2-tzvp/eva.hickle
    /home/runner/work/atomistic-cookbook/atomistic-cookbook/.nox/hamiltonian-qm7/lib/python3.11/site-packages/mlelec/utils/twocenter_utils.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
      return torch.tensor(matrix)[idx][:, idx]




.. GENERATED FROM PYTHON SOURCE LINES 242-273

Computing Features that can Learn Hamiltonian Targets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

As discussed above, the Hamiltonian matrix elements
are dependent on single atom centers and two centers
for pairwise interactions.
To address this, we extend the equivariant SOAP-based
features for the atom-centered desciptors
to a descriptor capable of describing multiple atomic centers and
their connectivities, giving rise to the equivariant pairwise descriptor
which simultaneously characterizes the environments for pairs of atoms
in a given system. Our `Periodic Hamiltonian Model Example
<https://atomistic-cookbook.org/examples/periodic-hamiltonian/periodic-hamiltonian.html>`__
discusses the construction of these descriptors in greater detail.
To construct these atom- and pair-centered features we use our
in house library `featomic <https://github.com/metatensor/featomic/>`_
for each structure in the our dataset using the hyperparameters
defined below. The features are constructed starting from a
description of a structure in terms of atom density, for which we
define the width of the overlaying Gaussians in the
``density`` hyperparameter. The features are
discretized on a basis of spherical harmonics,
which consist of a radial and an angular part,
which can be specified in the ``basis`` hyperparameter. The ``cutoff``
hyperparameter controls the extent of the atomic environment.
For the simple example we demonstrate here,
the atom and pairwise features have very similar hyperparameters,
except for the cutoff radius, which is larger for pairwise
features to include many pairs that describe the individual
atom-atom interaction.


.. GENERATED FROM PYTHON SOURCE LINES 273-299

.. code-block:: Python


    hypers = {
        "cutoff": {"radius": 2.5, "smoothing": {"type": "ShiftedCosine", "width": 0.1}},
        "density": {"type": "Gaussian", "width": 0.3},
        "basis": {
            "type": "TensorProduct",
            "max_angular": 4,
            "radial": {"type": "Gto", "max_radial": 5},
        },
    }

    hypers_pair = {
        "cutoff": {"radius": 3.0, "smoothing": {"type": "ShiftedCosine", "width": 0.1}},
        "density": {"type": "Gaussian", "width": 0.3},
        "basis": {
            "type": "TensorProduct",
            "max_angular": 4,
            "radial": {"type": "Gto", "max_radial": 5},
        },
    }

    features = compute_features_for_target(
        ml_data, device=DEVICE, hypers=hypers, hypers_pair=hypers_pair
    )
    ml_data._set_features(features)








.. GENERATED FROM PYTHON SOURCE LINES 300-311

Prepare Dataloaders
^^^^^^^^^^^^^^^^^^^
To efficiently feed data into the model during training,
we use data loaders. These handle batching and shuffling
to optimize training performance. ``get_dataloader``
creates data loaders for training, validation and testing.
The ``model_return="blocks"`` argument determines that the
model targets the different blocks that the Hamiltonian is
decomposed into and the ``batch_size`` argument defines the
number of samples per batch for the batch-wise training.


.. GENERATED FROM PYTHON SOURCE LINES 311-316

.. code-block:: Python


    train_dl, val_dl, test_dl = get_dataloader(
        ml_data, model_return="blocks", batch_size=BATCH_SIZE
    )








.. GENERATED FROM PYTHON SOURCE LINES 317-329

Prepare Training
^^^^^^^^^^^^^^^^

Next, we set up our linear model that predicts the Hamiltonian
matrices, using ``LinerTargetModel``. To improve the model
convergence, we first start with a symmetry-adapted ridge regression
targeting the Hamiltonian matrices from the STO-3G basis QM
calculation using the ``fit_ridge_analytical`` function.
This provides us a more reliable set of weights to initialise the
fine-tuning rather than starting from any random guess,
effectively saving us training time by starting the training process
closer to the desired minumum.

.. GENERATED FROM PYTHON SOURCE LINES 329-356

.. code-block:: Python



    model = LinearTargetModel(
        dataset=ml_data, nlayers=1, nhidden=16, bias=False, device=DEVICE
    )

    pred_ridges, ridges = model.fit_ridge_analytical(
        alpha=np.logspace(-8, 3, 12),
        cv=3,
        set_bias=False,
    )

    pred_fock = model.forward(
        ml_data.feat_train,
        return_type="tensor",
        batch_indices=ml_data.train_idx,
        ridge_fit=True,
        add_noise=NOISE,
    )

    with io.capture_output() as captured:
        all_mfs, fockvars = instantiate_mf(
            ml_data,
            fock_predictions=None,
            batch_indices=list(range(len(ml_data.structures))),
        )








.. GENERATED FROM PYTHON SOURCE LINES 357-377

Training: Indirect learning of the MO energies
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Now rather than explicitly targeting the Hamiltonian matrix,
we instead treat it as an intermediate layer in our framework, where
the model predicts the Hamiltonian, the model weights, however,
are subsequently fine-tuned by backpropagating
a loss on a derived molecular property of the Hamiltonian such as
the MO energies but from the larger def-TZVP basis instead of the
STO-3G basis.

Before fine-tuning the model we preconditioned
by a ridge regression fit of our data,
we set up the loss function to target the MO energies,
as well as the optimizer
and the learning rate scheduler for our model. We use a customized
mean squared error (MSE) loss function that guides the learning and
``Adam`` optimizer that performs a stochastic gradient descent
that minimizes the error. The scheduler reduces the learning
rate by the given factor if the validation loss plateaus.

.. GENERATED FROM PYTHON SOURCE LINES 377-386

.. code-block:: Python


    loss_fn = mlmetrics.mse_per_atom
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        factor=0.5,
        patience=3,  # 20,
    )








.. GENERATED FROM PYTHON SOURCE LINES 387-389

We use a ``Trainer`` class to encapsulate all training logic.
It manages the training and validation loops.

.. GENERATED FROM PYTHON SOURCE LINES 389-391

.. code-block:: Python

    trainer = Trainer(model, optimizer, scheduler, DEVICE)








.. GENERATED FROM PYTHON SOURCE LINES 392-393

Define necessary arguments for the training and validation process.

.. GENERATED FROM PYTHON SOURCE LINES 393-404

.. code-block:: Python

    fit_args = {
        "ml_data": ml_data,
        "all_mfs": all_mfs,
        "loss_fn": loss_fn,
        "weight_eva": 1,
        "weight_dipole": 0,
        "weight_polar": 0,
        "ORTHOGONAL": ORTHOGONAL,
        "upscale": True,
    }








.. GENERATED FROM PYTHON SOURCE LINES 405-410

With these steps complete, we can now train the model.
It begins training and validation
using the structured molecular data, features, and defined parameters.
The ``fit`` function returns the training and validation losses for
each epoch, which we can then use to plot the `loss versus epoch` curve.

.. GENERATED FROM PYTHON SOURCE LINES 410-424

.. code-block:: Python


    history = trainer.fit(
        train_dl,
        val_dl,
        NUM_EPOCHS,
        EARLY_STOP_CRITERION,
        FOLDER_NAME,
        VERBOSE,
        DUMP_HIST,
        **fit_args,
    )

    np.save(f"{FOLDER_NAME}/model_output/loss_stats.npy", history)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|                                                                                           | 0/100 [00:00<?, ?it/s]      0%|                                               | 0/100 [00:02<?, ?it/s, train_loss=0.0443, Val_loss=0.0101, lr=0.1]Checkpoint saved to output/ethane_eva/model_output/model_epoch0.pt
      1%|▍                                      | 1/100 [00:02<04:45,  2.89s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      2%|▊                                      | 2/100 [00:05<04:43,  2.90s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      3%|█▏                                     | 3/100 [00:08<04:41,  2.90s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      4%|█▌                                     | 4/100 [00:11<04:40,  2.92s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      5%|█▉                                     | 5/100 [00:14<04:38,  2.94s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      6%|██▎                                    | 6/100 [00:17<04:34,  2.92s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      7%|██▋                                    | 7/100 [00:20<04:31,  2.92s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      8%|███                                    | 8/100 [00:23<04:29,  2.93s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]      9%|███▌                                   | 9/100 [00:26<04:24,  2.90s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]     10%|███▊                                  | 10/100 [00:29<04:21,  2.90s/it, train_loss=0.0443, Val_loss=0.0101, lr=0.1]     10%|███▋                                 | 10/100 [00:32<04:21,  2.90s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     11%|████                                 | 11/100 [00:32<04:18,  2.90s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     12%|████▍                                | 12/100 [00:34<04:15,  2.90s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     13%|████▊                                | 13/100 [00:37<04:13,  2.92s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     14%|█████▏                               | 14/100 [00:40<04:10,  2.91s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     15%|█████▌                               | 15/100 [00:43<04:09,  2.93s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     16%|█████▉                               | 16/100 [00:46<04:07,  2.95s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     17%|██████▎                              | 17/100 [00:49<04:04,  2.95s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     18%|██████▋                              | 18/100 [00:52<04:00,  2.93s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     19%|███████                              | 19/100 [00:55<03:56,  2.92s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     20%|███████▍                             | 20/100 [00:58<03:53,  2.92s/it, train_loss=1.3e-5, Val_loss=1.95e-5, lr=0.1]     20%|███████                            | 20/100 [01:01<03:53,  2.92s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     21%|███████▎                           | 21/100 [01:01<03:50,  2.92s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     22%|███████▋                           | 22/100 [01:04<03:47,  2.92s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     23%|████████                           | 23/100 [01:07<03:44,  2.92s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     24%|████████▍                          | 24/100 [01:10<03:42,  2.92s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     25%|████████▊                          | 25/100 [01:13<03:40,  2.95s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     26%|█████████                          | 26/100 [01:15<03:37,  2.94s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     27%|█████████▍                         | 27/100 [01:18<03:34,  2.93s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     28%|█████████▊                         | 28/100 [01:21<03:31,  2.93s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     29%|██████████▏                        | 29/100 [01:24<03:27,  2.93s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     30%|██████████▌                        | 30/100 [01:27<03:24,  2.93s/it, train_loss=1.11e-5, Val_loss=1.65e-5, lr=0.05]     30%|██████████▏                       | 30/100 [01:30<03:24,  2.93s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     31%|██████████▌                       | 31/100 [01:30<03:21,  2.92s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     32%|██████████▉                       | 32/100 [01:33<03:17,  2.91s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     33%|███████████▏                      | 33/100 [01:36<03:14,  2.91s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     34%|███████████▌                      | 34/100 [01:39<03:07,  2.85s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     35%|███████████▉                      | 35/100 [01:42<03:06,  2.87s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     36%|████████████▏                     | 36/100 [01:44<03:04,  2.88s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     37%|████████████▌                     | 37/100 [01:47<03:05,  2.94s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     38%|████████████▉                     | 38/100 [01:50<02:57,  2.87s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     39%|█████████████▎                    | 39/100 [01:53<02:54,  2.86s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     40%|█████████████▌                    | 40/100 [01:56<02:51,  2.86s/it, train_loss=5.97e-6, Val_loss=6.76e-6, lr=0.025]     40%|█████████████▏                   | 40/100 [01:59<02:51,  2.86s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     41%|█████████████▌                   | 41/100 [01:59<02:48,  2.86s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     42%|█████████████▊                   | 42/100 [02:02<02:46,  2.87s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     43%|██████████████▏                  | 43/100 [02:04<02:43,  2.86s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     44%|██████████████▌                  | 44/100 [02:07<02:40,  2.87s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     45%|██████████████▊                  | 45/100 [02:10<02:37,  2.86s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     46%|███████████████▏                 | 46/100 [02:13<02:35,  2.87s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     47%|███████████████▌                 | 47/100 [02:16<02:32,  2.87s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     48%|███████████████▊                 | 48/100 [02:19<02:29,  2.88s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     49%|████████████████▏                | 49/100 [02:22<02:26,  2.88s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     50%|████████████████▌                | 50/100 [02:25<02:24,  2.89s/it, train_loss=4.83e-6, Val_loss=5.88e-6, lr=0.0125]     50%|████████████████                | 50/100 [02:28<02:24,  2.89s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]Checkpoint saved to output/ethane_eva/model_output/model_epoch50.pt
     51%|████████████████▎               | 51/100 [02:28<02:21,  2.89s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     52%|████████████████▋               | 52/100 [02:30<02:19,  2.90s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     53%|████████████████▉               | 53/100 [02:33<02:15,  2.89s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     54%|█████████████████▎              | 54/100 [02:36<02:12,  2.89s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     55%|█████████████████▌              | 55/100 [02:39<02:09,  2.88s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     56%|█████████████████▉              | 56/100 [02:42<02:06,  2.88s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     57%|██████████████████▏             | 57/100 [02:45<02:04,  2.89s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     58%|██████████████████▌             | 58/100 [02:48<02:01,  2.88s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     59%|██████████████████▉             | 59/100 [02:51<01:57,  2.87s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     60%|███████████████████▏            | 60/100 [02:53<01:54,  2.87s/it, train_loss=4.77e-6, Val_loss=5.73e-6, lr=0.00625]     60%|███████████████████▏            | 60/100 [02:56<01:54,  2.87s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     61%|███████████████████▌            | 61/100 [02:56<01:49,  2.81s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     62%|███████████████████▊            | 62/100 [02:59<01:47,  2.83s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     63%|████████████████████▏           | 63/100 [03:02<01:45,  2.85s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     64%|████████████████████▍           | 64/100 [03:05<01:42,  2.85s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     65%|████████████████████▊           | 65/100 [03:08<01:40,  2.87s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     66%|█████████████████████           | 66/100 [03:11<01:37,  2.87s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     67%|█████████████████████▍          | 67/100 [03:13<01:35,  2.88s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     68%|█████████████████████▊          | 68/100 [03:16<01:32,  2.88s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     69%|██████████████████████          | 69/100 [03:19<01:29,  2.89s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     70%|██████████████████████▍         | 70/100 [03:22<01:26,  2.88s/it, train_loss=4.38e-6, Val_loss=5.53e-6, lr=0.00156]     70%|██████████████████████▍         | 70/100 [03:25<01:26,  2.88s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     71%|██████████████████████▋         | 71/100 [03:25<01:23,  2.88s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     72%|███████████████████████         | 72/100 [03:28<01:20,  2.89s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     73%|███████████████████████▎        | 73/100 [03:31<01:18,  2.90s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     74%|███████████████████████▋        | 74/100 [03:34<01:15,  2.90s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     75%|████████████████████████        | 75/100 [03:37<01:12,  2.89s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     76%|████████████████████████▎       | 76/100 [03:40<01:09,  2.91s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     77%|████████████████████████▋       | 77/100 [03:43<01:07,  2.93s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     78%|████████████████████████▉       | 78/100 [03:46<01:04,  2.94s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     79%|█████████████████████████▎      | 79/100 [03:48<01:02,  2.96s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     80%|█████████████████████████▌      | 80/100 [03:51<00:59,  2.96s/it, train_loss=4.27e-6, Val_loss=5.5e-6, lr=0.000781]     80%|████████████████████████▊      | 80/100 [03:54<00:59,  2.96s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     81%|█████████████████████████      | 81/100 [03:54<00:56,  2.95s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     82%|█████████████████████████▍     | 82/100 [03:57<00:52,  2.94s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     83%|█████████████████████████▋     | 83/100 [04:00<00:49,  2.94s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     84%|██████████████████████████     | 84/100 [04:03<00:46,  2.93s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     85%|██████████████████████████▎    | 85/100 [04:06<00:43,  2.91s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     86%|██████████████████████████▋    | 86/100 [04:09<00:40,  2.89s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     87%|██████████████████████████▉    | 87/100 [04:12<00:37,  2.90s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     88%|███████████████████████████▎   | 88/100 [04:15<00:34,  2.90s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     89%|███████████████████████████▌   | 89/100 [04:18<00:31,  2.90s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     90%|███████████████████████████▉   | 90/100 [04:20<00:28,  2.90s/it, train_loss=4.25e-6, Val_loss=5.48e-6, lr=0.000391]     90%|████████████████████████████▊   | 90/100 [04:23<00:28,  2.90s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     91%|█████████████████████████████   | 91/100 [04:23<00:26,  2.89s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     92%|█████████████████████████████▍  | 92/100 [04:26<00:23,  2.91s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     93%|█████████████████████████████▊  | 93/100 [04:29<00:20,  2.92s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     94%|██████████████████████████████  | 94/100 [04:32<00:17,  2.91s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     95%|██████████████████████████████▍ | 95/100 [04:35<00:14,  2.92s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     96%|██████████████████████████████▋ | 96/100 [04:38<00:11,  2.92s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     97%|███████████████████████████████ | 97/100 [04:41<00:08,  2.92s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]     98%|███████████████████████████████▎| 98/100 [04:44<00:05,  2.91s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]Early stopping triggered after 98 epochs.
     98%|███████████████████████████████▎| 98/100 [04:47<00:05,  2.93s/it, train_loss=4.22e-6, Val_loss=5.52e-6, lr=9.77e-5]




.. GENERATED FROM PYTHON SOURCE LINES 425-430

Plot Loss
^^^^^^^^^

With the help of ``plot_losses`` function we can conveniently
plot the training and validation losses.

.. GENERATED FROM PYTHON SOURCE LINES 430-433

.. code-block:: Python


    plot_losses(history, save=True, savename=f"{FOLDER_NAME}/loss_vs_epoch.pdf")




.. image-sg:: /examples/hamiltonian-qm7/images/sphx_glr_hamiltonian-qm7_001.png
   :alt: Training Losses, Validation Losses
   :srcset: /examples/hamiltonian-qm7/images/sphx_glr_hamiltonian-qm7_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 434-443

Parity Plot
^^^^^^^^^^^
We then evaluate the prediction of the fine-tuned model on
the MO energies by comparing it the with the MO energies
from the reference def2-TZVP calculation. The parity plot
below shows the performance of our model on the test
dataset. ML predictions are shown with blue points and
the corresponding MO energies from the STO-3G basis are
are shown in grey.

.. GENERATED FROM PYTHON SOURCE LINES 443-536

.. code-block:: Python



    def plot_parity_property(molecule_data, propert="eva", orthogonal=True):
        plt.figure()
        plt.plot([-25, 20], [-25, 20], "k--")
        plt.plot(
            torch.cat(
                [
                    molecule_data.lb_target[propert][i][
                        : molecule_data.target[propert][i].shape[0]
                    ]
                    for i in ml_data.test_idx
                ]
            )
            .detach()
            .numpy()
            .flatten()
            * Hartree,
            torch.cat([molecule_data.target[propert][i] for i in ml_data.test_idx])
            .detach()
            .numpy()
            .flatten()
            * Hartree,
            "o",
            alpha=0.7,
            color="gray",
            markeredgecolor="black",
            markeredgewidth=0.2,
            label="STO-3G",
        )
        f_pred = model.forward(
            ml_data.feat_test,
            return_type="tensor",
            batch_indices=ml_data.test_idx,
        )

        if propert == "eva":
            prop = compute_eigvals(
                ml_data, f_pred, range(len(ml_data.test_idx)), orthogonal=orthogonal
            )
            prop = torch.tensor([p for pro in prop for p in pro]).detach().numpy()
            propertunit = "eV"
        elif propert == "dip":
            prop = compute_dipole_moment(
                [molecule_data.structures[i] for i in ml_data.test_idx],
                f_pred,
                orthogonal=orthogonal,
            )
            prop = torch.tensor([p for pro in prop for p in pro]).detach().numpy()
            propertunit = "a.u."
        elif propert == "pol":
            prop = compute_polarisability(
                [molecule_data.structures[i] for i in ml_data.test_idx],
                f_pred,
                orthogonal=orthogonal,
            )
            prop = prop.flatten().detach().numpy()
            propertunit = "a.u."
        else:
            print("Property not implemented")

        plt.plot(
            torch.cat(
                [
                    molecule_data.lb_target[propert][i][
                        : molecule_data.target[propert][i].shape[0]
                    ]
                    for i in ml_data.test_idx
                ]
            )
            .detach()
            .numpy()
            .flatten()
            * Hartree,
            prop * Hartree,
            "o",
            alpha=0.7,
            color="royalblue",
            markeredgecolor="black",
            markeredgewidth=0.2,
            label="ML",
        )
        plt.ylim(-25, 20)
        plt.xlim(-25, 20)
        plt.xlabel(f"Reference {propert} ({propertunit})")
        plt.ylabel(f"Predicted {propert} ({propertunit})")
        plt.savefig(f"{FOLDER_NAME}/parity_{propert}.pdf")
        plt.legend()
        plt.show()


    plot_parity_property(molecule_data, propert="eva", orthogonal=ORTHOGONAL)




.. image-sg:: /examples/hamiltonian-qm7/images/sphx_glr_hamiltonian-qm7_002.png
   :alt: hamiltonian qm7
   :srcset: /examples/hamiltonian-qm7/images/sphx_glr_hamiltonian-qm7_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 537-542

We can observe from the parity plot that even with a
minimal basis parametrisation, the model is able to reproduce
the large basis MO energies with good accuracy. Thus, using
an indirect model, makes it possible to promote the model
accuracy to a higher level of theory, at no additional cost.

.. GENERATED FROM PYTHON SOURCE LINES 545-562

2. Example of Targeting Multiple Properties
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In principle we can also target multiple properties for
the indirect training. While MO energies can be computed
by simply diagonalizing the Hamiltonian matrix, some
properties like the dipole moment require
the position operator integral and its derivative if
we want to backpropagate the loss. We therefore interface
our ML model with an electronic structure code that supports
automatic differentiation, `PySCFAD <https://github.com/fishjojo/pyscfad>`_,
an end-to-end auto-differentiable version of PySCF. By
doing so we delegate the computation of properties to PySCFAD,
which provides automatic differentiation of observables with
respect to the intermediate Hamiltonian. In particular, we will now
indirectly target the dipole moment and polarisability along
with the MO energies from a large basis reference calculation

.. GENERATED FROM PYTHON SOURCE LINES 565-574

Get Data and Prepare Data Set
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
In our last example even though we show an indirect ML model
that was trained on a homogenous dataset of different
configurations of ethane, we can also easily extend the
framework to use  a much diverse dataset such as the
`QM7 dataset <http://quantum-machine.org/datasets/>`_.
For our next example we select a subset of 150 structures from
this dataset that consists of only C, H, N and O atoms.

.. GENERATED FROM PYTHON SOURCE LINES 576-603

Set parameters for training
^^^^^^^^^^^^^^^^^^^^^^^^^^^
Set the parameters for the training, including the dataset set size
and split, the batch size, learning rate and weights for the individual
components of eigenvalues, dipole and polarisability.
We additionally define a folder name, in which the results are saved.
Optionally, noise can be added to the ridge regression fit.

Here, we now need to provide different weights
for the different targets (eigenvalues
:math:`\epsilon`, the dipole moment
:math:`\mu`, and polarisability
:math:`\alpha`), which we will use when computing the loss :math:`\mathcal{L}`.

.. math::
   \mathcal{L}_{\epsilon,\mu,\alpha} = & \;
   \frac{\omega_{\epsilon}}{N} \sum_{n=1}^{N} \frac{1}{O_n} \
   \sum_{o=1}^{O_n} \left( \epsilon_{no} - \tilde{\epsilon}_{no} \right)^2 \
    + \frac{\omega_{\mu}}{N} \sum_{n=1}^{N} \frac{1}{N_A^2} \
   \sum_{m=1}^{N_A} \left( \mu_{nm} - \tilde{\mu}_{nm} \right)^2 \\
   & + \frac{\omega_{\alpha}}{N} \sum_{n=1}^{N} \frac{1}{N_A^2} \
   \sum_{m=1}^{N_A} \left( \alpha_{nm} - \tilde{\alpha}_{nm} \right)^2

where
:math:`N` is the number of training points,
:math:`O_n` is the number of MO orbitals in the nth molecule,
:math:`N_A` is the number of atoms :math:`i`.

.. GENERATED FROM PYTHON SOURCE LINES 605-612

The weights
:math:`\omega` in the loss are based on the magnitude of errors
for different properties, where at the end we want each of them to
contribute equally to the loss.
The following values worked well for the QM7 example, but of
course depending on the system that one investigates another set
of weights might work better.

.. GENERATED FROM PYTHON SOURCE LINES 614-620

.. code-block:: Python

    NUM_FRAMES = 150
    LR = 1e-3
    W_EVA = 1e4
    W_DIP = 1e3
    W_POL = 1e2








.. GENERATED FROM PYTHON SOURCE LINES 621-637

Create Datasets
^^^^^^^^^^^^^^^

We use the dataloader of the
`mlelec package (qm7 branch) <https://github.com/curiosity54/mlelec/tree/qm7>`_,
and load the ethane
dataset for the defined number of slices.
First, we load all relavant data (geometric structures,
auxiliary matrices -overlap and orbitals-, and
targets -fock, dipole moment, and polarisablity-) into a molecule dataset.
We do this for the minimal (STO-3G), as well as a larger basis (lb, def2-TZVP).
The larger basis has additional basis functions on the valence electrons.
The dataset, we can then load into our dataloader ```ml_data``, together with some
settings on how we want to sample data from the dataloader.
Finally, we define the desired dataset split for training, validation,
and testing from the parameters defined in example 1.

.. GENERATED FROM PYTHON SOURCE LINES 637-664

.. code-block:: Python


    molecule_data = MoleculeDataset(
        mol_name="qm7",
        use_precomputed=False,
        path="data/qm7",
        aux_path="data/qm7/sto-3g",
        frame_slice=slice(0, NUM_FRAMES),
        device=DEVICE,
        aux=["overlap", "orbitals"],
        lb_aux=["overlap", "orbitals"],
        target=["fock", "eva", "dip", "pol"],
        lb_target=["fock", "eva", "dip", "pol"],
    )

    ml_data = MLDataset(
        molecule_data=molecule_data,
        device=DEVICE,
        model_strategy="coupled",
        shuffle=True,
        shuffle_seed=SHUFFLE_SEED,
        orthogonal=ORTHOGONAL,
    )

    ml_data._split_indices(
        train_frac=TRAIN_FRAC, val_frac=VALIDATION_FRAC, test_frac=TEST_FRAC
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Loading structures
    data/qm7/sto-3g/fock.hickle
    data/qm7/sto-3g/eva.hickle
    data/qm7/sto-3g/dip.hickle
    data/qm7/sto-3g/pol.hickle
    data/qm7/def2-tzvp/fock.hickle
    data/qm7/def2-tzvp/eva.hickle
    data/qm7/def2-tzvp/dip.hickle
    data/qm7/def2-tzvp/pol.hickle
    /home/runner/work/atomistic-cookbook/atomistic-cookbook/.nox/hamiltonian-qm7/lib/python3.11/site-packages/mlelec/utils/twocenter_utils.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).
      return torch.tensor(matrix)[idx][:, idx]




.. GENERATED FROM PYTHON SOURCE LINES 665-673

Compute Features
^^^^^^^^^^^^^^^^
The feature hyperparameters here are similar to the
ones we used for our previous training, except the `cutoff`.
For a dataset like QM7 which contains molecules with over
15 atoms, we need a slightly larger cutoff than the ones
we used in case of ethane. We choose a cutoff of 3 and 5
for the atom-centred and pair-centred features respectively.

.. GENERATED FROM PYTHON SOURCE LINES 673-693

.. code-block:: Python


    hypers = {
        "cutoff": {"radius": 3, "smoothing": {"type": "ShiftedCosine", "width": 0.1}},
        "density": {"type": "Gaussian", "width": 0.3},
        "basis": {
            "type": "TensorProduct",
            "max_angular": 4,
            "radial": {"type": "Gto", "max_radial": 5},
        },
    }

    hypers_pair = {
        "cutoff": {"radius": 5, "smoothing": {"type": "ShiftedCosine", "width": 0.1}},
        "density": {"type": "Gaussian", "width": 0.3},
        "basis": {
            "type": "TensorProduct",
            "max_angular": 4,
            "radial": {"type": "Gto", "max_radial": 5},
        },
    }







.. GENERATED FROM PYTHON SOURCE LINES 694-704

.. code-block:: python

      features = compute_features_for_target(
          ml_data, device=DEVICE, hypers=hypers, hypers_pair=hypers_pair
      )
      ml_data._set_features(features)

      train_dl, val_dl, test_dl = get_dataloader(
          ml_data, model_return="blocks", batch_size=BATCH_SIZE
      )

.. GENERATED FROM PYTHON SOURCE LINES 706-712

Depending on the diversity of the structures in the datasets, it may
happen that some blocks are empty, because certain structural
features are only present in certain structures (e.g. if we
would have some organic molecules with oxygen and some without).
We drop these blocks, so that the dataloader does not
try to load them during training.

.. GENERATED FROM PYTHON SOURCE LINES 714-722

.. code-block:: python

  ml_data.target_train, ml_data.target_val, ml_data.target_test = drop_zero_blocks(
  ml_data.target_train, ml_data.target_val, ml_data.target_test)

  ml_data.feat_train, ml_data.feat_val, ml_data.feat_test = drop_zero_blocks(
  ml_data.feat_train, ml_data.feat_val, ml_data.feat_test)


.. GENERATED FROM PYTHON SOURCE LINES 725-729

Prepare training
^^^^^^^^^^^^^^^^

Here again we first fit a ridge regression model to the data.

.. GENERATED FROM PYTHON SOURCE LINES 731-757

.. code-block:: python

      model = LinearTargetModel(
          dataset=ml_data, nlayers=1, nhidden=16, bias=False, device=DEVICE
      )

      pred_ridges, ridges = model.fit_ridge_analytical(
          alpha=np.logspace(-8, 3, 12),
          cv=3,
          set_bias=False,
      )

      pred_fock = model.forward(
          ml_data.feat_train,
          return_type="tensor",
          batch_indices=ml_data.train_idx,
          ridge_fit=True,
          add_noise=NOISE,
      )

      with io.capture_output() as captured:
          all_mfs, fockvars = instantiate_mf(
              ml_data,
              fock_predictions=None,
              batch_indices=list(range(len(ml_data.structures))),
          )

.. GENERATED FROM PYTHON SOURCE LINES 759-766

Training parameters and training
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For finetuning on multiple targets we again
define a loss function, optimizer and scheduler.
We also define the necessary arguments for training
and validation.

.. GENERATED FROM PYTHON SOURCE LINES 768-777

.. code-block:: python

      loss_fn = mlmetrics.mse_per_atom
      optimizer = torch.optim.Adam(model.parameters(), lr=LR)
      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
          optimizer,
          factor=0.5,
          patience=10,
          )

.. GENERATED FROM PYTHON SOURCE LINES 779-811

.. code-block:: python

      # Initialize trainer
      trainer = Trainer(model, optimizer, scheduler, DEVICE)

      # Define necessary arguments for the training and validation process
      fit_args = {
          "ml_data": ml_data,
          "all_mfs": all_mfs,
          "loss_fn": loss_fn,
          "weight_eva": W_EVA,
          "weight_dipole": W_DIP,
          "weight_polar": W_POL,
          "ORTHOGONAL": ORTHOGONAL,
          "upscale": True,
      }


      # Train and validate
      history = trainer.fit(
          train_dl,
          val_dl,
          200,
          EARLY_STOP_CRITERION,
          FOLDER_NAME,
          VERBOSE,
          DUMP_HIST,
          **fit_args,
      )

      # Save the loss history
      np.save(f"{FOLDER_NAME}/model_output/loss_stats.npy", history)

.. GENERATED FROM PYTHON SOURCE LINES 813-815

This training can take some time to converge fully.
We, therefore try to load a previously trained model.

.. GENERATED FROM PYTHON SOURCE LINES 817-823

Evaluating the trained model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: python

      model.load_state_dict(torch.load("output/qm7_eva_dip_pol/best_model.pt"))

.. GENERATED FROM PYTHON SOURCE LINES 825-836

Plot loss
^^^^^^^^^

.. figure:: loss_vs_epoch.png
   :alt: loss versus epoch curves for training and
         validation losses.  The MSE on MO energies, dipole moments
         and polarisability are shown separately.
The plot here shows the Loss versus Epoch curves
for training and validation losses.  The MSE on
MO energies, dipole moments and polarisability
are shown separately.

.. GENERATED FROM PYTHON SOURCE LINES 838-856

Parity plot
^^^^^^^^^^^

.. figure:: parity_plots_combined.png
   :alt: Performance of the indirect model on the QM7 test
          dataset, for the (a) MO energy (b) dipole moments and (c)
          polarizability. Targets are computed with the def2-TZVP
          basis. Gray circles correspond to the values obtained from
          STO-3G calculations, while the blue ones correspond to val-
          ues computed from minimal-basis Hamiltonians predicted by
          the ML model.
We finally show the performance of the finetuned model
that target the MO energies, dipole moments and polarisbaility
of from a def2-TZVP calculation on the QM7 test
dataset. Gray circles correspond to the values obtained from
STO-3G calculations, while the blue ones correspond to values
computed from the reduced-basis Hamiltonians predicted by
the ML model.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (5 minutes 35.895 seconds)


.. _sphx_glr_download_examples_hamiltonian-qm7_hamiltonian-qm7.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: hamiltonian-qm7.ipynb <hamiltonian-qm7.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: hamiltonian-qm7.py <hamiltonian-qm7.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

        :download:`Download recipe: hamiltonian-qm7.zip <hamiltonian-qm7.zip>`
    

.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
